from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import timedelta, datetime
from kubernetes.client import models as k8s
from airflow.models.baseoperator import chain

# Create k8s storage mount 

logs_volume_mount = k8s.V1VolumeMount(name="logs-volume", mount_path="/lsst-backup-logs", sub_path=None, read_only=False,)
logs_volume = k8s.V1Volume(
    name="logs-volume",
    host_path=k8s.V1HostPathVolumeSource(path='/lsst-backup-logs', type="DirectoryOrCreate"),
)

# Define default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

# dict to hold new CSV files for multiple bucket names
new_csvs = {}

def dl_bucket_names(url):
    import json
    import requests
    bucket_names = []
    r = requests.get(url)
    buckets = json.loads(r.text)
    for bucket in buckets:
        bucket_names.append(bucket['name'])
    print(f'Bucket names found: {bucket_names}')
    return bucket_names

bucket_names = dl_bucket_names('https://raw.githubusercontent.com/lsst-uk/csd3-echo-somerville/main/echo-side/bucket_names/bucket_names.json')

# Instantiate the DAG
with DAG(
    'check_uploads',
    default_args=default_args,
    description='Open list generated by monitor.py and validate uploads detailed in new CSV files.',
    schedule_interval='0 14 * * *', # set to  2pm 
    start_date=datetime(2024, 1, 1, 14, 0, 0), 
    catchup=False,
) as dag:



###
# def get_new_csvs(bucket_name):
#     new_csvs[bucket_name] = []
#     with open(''.join([f'/lsst-backup-logs/new-csv-files-{bucket_name}','{{ ds_nodash }}','.txt']), 'r') as f:
#         for line in f:
#             new_csvs[bucket_name].append(line.strip())
#     if len(new_csvs[bucket_name]) == 0:
#         del new_csvs[bucket_name]


    # get_new_csvs_task = [ PythonOperator(
    #     task_id=f'get_new_csvs-{bucket_name}',
    #     python_callable=get_new_csvs,
    #     op_args=[bucket_name],
    # ) for bucket_name in bucket_names ]

        # get_new_csvs_task,

###



###
# check_uploads = []

#     for bucket_new_csvs in new_csvs.items():
#         bucket_name = bucket_new_csvs[0]
#         new_csvs_list = bucket_new_csvs[1]
#         for csv in new_csvs_list:
#             check_uploads.append(
#                 KubernetesPodOperator(
#                 task_id=f'check_{csv}',
#                 image='ghcr.io/lsst-uk/csd3-echo-somerville:latest',
#                 cmds=['./entrypoint.sh'],
#                 arguments=['python', 'csd3-echo-somerville/scripts/check_upload.py', bucket_name, csv],
#                 env_vars={
#                     'ECHO_S3_ACCESS_KEY': Variable.get("ECHO_S3_ACCESS_KEY"),
#                     'ECHO_S3_SECRET_KEY': Variable.get("ECHO_S3_SECRET_KEY"),
#                 },
#                 volumes=[logs_volume],
#                 volume_mounts=[logs_volume_mount],
#                 get_logs=True,
#                 )
#             )
###