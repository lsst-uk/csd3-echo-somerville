{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8006c92e-bb36-477a-a310-cb025658eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f15ea6-9859-448f-b2c4-d90dd741f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bucket_manager as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94646936-e186-46ca-81c9-f1ed5a443ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate timing\n",
    "start = datetime.now()\n",
    "# Set the source directory, bucket name, and destination directory\n",
    "subdir = 'dmu3' # change subdir to sys.argv[1] in script\n",
    "source_dir = f\"/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/{subdir}\"\n",
    "log = f\"{'-'.join(source_dir.split('/')[-3:])}-files.csv\"\n",
    "destination_dir = f\"ip005-ras81-lsst-ir-fusion/{subdir}\" \n",
    "folders = []\n",
    "folder_files = []\n",
    "ncores = 1 # change to adjust number of CPUs (= number of concurrent connections)\n",
    "perform_checksum = True\n",
    "upload_checksum = False\n",
    "dryrun = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c91ac5-e798-4a94-8149-a43bb75350ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add titles to log file\n",
    "with open(log, 'w') as logfile: # elsewhere open(log, 'a')\n",
    "    logfile.write('LOCAL_FOLDER,LOCAL_PATH,FILE_SIZE,BUCKET_NAME,DESTINATION_KEY,CHECKSUM,CHECKSUM_SIZE,CHECKSUM_KEY\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2f300e-12d4-4519-a5d9-042325d0f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bucket\n",
    "s3_host = 'echo.stfc.ac.uk'\n",
    "keys = bm.get_keys(os.sep.join([os.environ['HOME'],'lsst_keys.json']))\n",
    "access_key = keys['access_key']\n",
    "secret_key = keys['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56867cc2-4584-461e-b74c-0a544f93d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bm.get_client(access_key, secret_key, s3_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e800f7-eb7c-41af-aeee-5e16b4620a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x2ab6d582a990>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b78d323-91e0-48f5-b219-2192899c4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'csd3-backup-test'\n",
    "if dryrun:\n",
    "    mybucket = 'dummy_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2861a41-1092-42cd-89f6-2b63671cca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket exists: csd3-backup-test\n"
     ]
    }
   ],
   "source": [
    "if bucket_name not in bm.bucket_list(client):\n",
    "    if not dryrun:\n",
    "            bm.create_bucket(client,bucket_name)\n",
    "            print(f'Added bucket: {bucket_name}')\n",
    "else:\n",
    "    if not dryrun:\n",
    "        print(f'Bucket exists: {bucket_name}')\n",
    "        #sys.exit('Bucket exists.')\n",
    "    else:\n",
    "        print(f'Bucket exists: {bucket_name}')\n",
    "        print('dryrun = True, so continuing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0545b84-49bb-40f5-b2c0-bd1f53081b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip005-ras81-lsst-ir-fusion/dmu3/readme.md',\n",
       " 'lsst-ir-fusion--f-files.csv',\n",
       " 'ras81-lsst-ir-fusion-dmu3-files.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_objects = bm.object_list(client,bucket_name)\n",
    "current_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9fdf2e0-2db2-408c-ad63-bd6ebf75eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket(s3_host,access_key,secret_key,bucket_name,folder,filename,object_name,perform_checksum,upload_checksum,dryrun):\n",
    "    client = bm.get_client(access_key, secret_key, s3_host)\n",
    "    file_data = open(filename, 'rb')\n",
    "    if perform_checksum:\n",
    "        checksum = hashlib.md5(file_data.read()).hexdigest().encode('utf-8')\n",
    "        if upload_checksum and not dryrun:\n",
    "            checksum_key = object_name + '.checksum'\n",
    "            #create checksum object\n",
    "            key = bucket.new_key(checksum_key)\n",
    "            key.set_contents_from_string(checksum)\n",
    "    \"\"\"\n",
    "    - Upload the file to the bucket\n",
    "    \"\"\"\n",
    "    if not dryrun:\n",
    "        client.upload_fileobj(file_data,bucket_name,object_name)\n",
    "        # key = bucket.new_key(object_name)\n",
    "        # key.set_contents_from_filename(filename)\n",
    "\n",
    "    \"\"\"\n",
    "        report actions\n",
    "        CSV formatted\n",
    "        header: LOCAL_FOLDER,LOCAL_PATH,FILE_SIZE,BUCKET_NAME,DESTINATION_KEY,CHECKSUM,CHECKSUM_SIZE,CHECKSUM_KEY\n",
    "    \"\"\"\n",
    "    return_string = f'{folder},{filename},{os.stat(filename).st_size},{bucket_name},{object_name}'\n",
    "    if perform_checksum and upload_checksum:\n",
    "        return_string += f',{checksum},{len(checksum)},{checksum_key}'\n",
    "    elif perform_checksum:\n",
    "        return_string += f',{checksum},n/a,n/a'\n",
    "    else:\n",
    "        return_string += ',n/a,n/a,n/a'\n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1edc6ff-44de-4339-8f36-9bec9d880205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(log,folder,file_count,total_size,folder_start,folder_end,upload_checksum):\n",
    "    elapsed = folder_end - folder_start\n",
    "    print(f'Finished folder {folder}, elapsed time = {elapsed}')\n",
    "    elapsed_seconds = elapsed.seconds + elapsed.microseconds / 1e6\n",
    "    avg_file_size = total_size / file_count / 1024**2\n",
    "    if not upload_checksum:\n",
    "        print(f'{file_count} files (avg {avg_file_size:.2f} MiB/file) uploaded in {elapsed_seconds:.2f} seconds, {elapsed_seconds/file_count:.2f} s/file',flush=True)\n",
    "        print(f'{total_size / 1024**2:.2f} MiB uploaded in {elapsed_seconds:.2f} seconds, {total_size / 1024**2 / elapsed_seconds:.2f} MiB/s',flush=True)\n",
    "    if upload_checksum:\n",
    "        checksum_size = 32*file_count # checksum byte strings are 32 bytes\n",
    "        total_size += checksum_size\n",
    "        file_count *= 2\n",
    "        print(f'{file_count} files (avg {avg_file_size:.2f} MiB/file) uploaded (including checksum files) in {elapsed_seconds:.2f} seconds, {elapsed_seconds/file_count:.2f} s/file',flush=True)\n",
    "        print(f'{total_size / 1024**2:.2f} MiB uploaded (including checksum files) in {elapsed_seconds:.2f} seconds, {total_size / 1024**2 / elapsed_seconds:.2f} MiB/s',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718eacea-e758-404d-a753-5178d3475055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(s3_host,access_key,secret_key, bucket_name, current_objects, source_dir, destination_dir, ncores, perform_checksum, upload_checksum, dryrun, log):\n",
    "    i = 0\n",
    "    #processed_files = []\n",
    "    with Pool(ncores) as pool: # use 4 CPUs by default - very little speed-up, might drop multiprocessing and parallelise at shell level\n",
    "        #recursive loop over local folder\n",
    "        for folder,subfolders,files in os.walk(source_dir):\n",
    "            # check folder isn't empty\n",
    "            if len(files) > 0:\n",
    "                # all files within folder\n",
    "                folder_files = [ os.sep.join([folder,filename]) for filename in files ]\n",
    "                # keys to files on s3\n",
    "                object_names = [ os.sep.join([destination_dir, os.path.relpath(filename, source_dir)]) for filename in folder_files ]\n",
    "                print(folder_files)\n",
    "                print(object_names)\n",
    "                # remove current objects - avoids reuploading\n",
    "                # could provide overwrite flag if this is desirable\n",
    "                for oni,on in enumerate(object_names):\n",
    "                    if on in current_objects:\n",
    "                        object_names.remove(on)\n",
    "                        del folder_files[oni]\n",
    "                print(folder_files)\n",
    "                print(object_names)\n",
    "                folder_start = datetime.now()\n",
    "                file_count = len(files)\n",
    "                print('check for symlinks')\n",
    "                for f in files:\n",
    "                    if os.path.islink(f):\n",
    "                        print(os.path.islink(f))\n",
    "                        raise Exception(\"Not dealing with symlinks here yet.\")\n",
    "                # upload files in parallel and log output\n",
    "                print(f'Uploading {file_count} files from {folder} using {ncores} processes.')\n",
    "                with open(log, 'a') as logfile:\n",
    "                    for result in pool.starmap(upload_to_bucket, zip(repeat(s3_host),repeat(access_key),repeat(secret_key), repeat(bucket_name), repeat(folder), folder_files, object_names, repeat(perform_checksum), repeat(upload_checksum), repeat(dryrun))):\n",
    "                        logfile.write(f'{result}\\n')\n",
    "                folder_end = datetime.now()\n",
    "                folder_files_size = np.sum(np.array([os.path.getsize(filename) for filename in folder_files]))\n",
    "                print_stats(log, folder, file_count, folder_files_size, folder_start, folder_end, upload_checksum)\n",
    "\n",
    "                # testing - stop after 1 folders\n",
    "                i+=1\n",
    "                if i == 1:\n",
    "                    break\n",
    "    # Upload log file\n",
    "    if not dryrun:\n",
    "        upload_to_bucket(s3_host,access_key,secret_key,bucket_name, '/', log, os.path.basename(log), False, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d857d-b6aa-44d4-980c-fa50949c76b4",
   "metadata": {},
   "source": [
    "# Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7c6502-ac58-4aa7-bc1b-7e5d92438bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at 2024-02-23 17:00:32.060236, elapsed time = 0:16:05.949593\n",
      "['/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/readme.md']\n",
      "['ip005-ras81-lsst-ir-fusion/dmu3/readme.md']\n",
      "[]\n",
      "[]\n",
      "check for symlinks\n",
      "Uploading 1 files from /rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3 using 1 processes.\n",
      "Finished folder /rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3, elapsed time = 0:00:00.002559\n",
      "1 files (avg 0.00 MiB/file) uploaded in 0.00 seconds, 0.00 s/file\n",
      "0.00 MiB uploaded in 0.00 seconds, 0.00 MiB/s\n",
      "Finished at 2024-02-23 17:01:33.082468, elapsed time = 0:17:06.971830\n"
     ]
    }
   ],
   "source": [
    "# Process the files in parallel\n",
    "print(f'Starting processing at {datetime.now()}, elapsed time = {datetime.now() - start}')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    process_files(s3_host,access_key,secret_key, bucket_name, current_objects, source_dir, destination_dir, ncores, perform_checksum, upload_checksum, dryrun, log)\n",
    "\n",
    "# Complete\n",
    "print(f'Finished at {datetime.now()}, elapsed time = {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e05569-5000-45b0-a163-593c8c2d4c9a",
   "metadata": {},
   "source": [
    "## Note to self: it's not going into subfolders anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f18f84-4616-46b2-b069-de01b8838a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
