{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8006c92e-bb36-477a-a310-cb025658eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42889a0a-95aa-4542-83d3-801a7a2b06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a3bed02-4acc-44c0-bbf7-e73e11e24478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bucket_manager as bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94646936-e186-46ca-81c9-f1ed5a443ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate timing\n",
    "start = datetime.now()\n",
    "# Set the source directory, bucket name, and destination directory\n",
    "subdir = 'dmu3' # change subdir to sys.argv[1] in script\n",
    "source_dir = f\"/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/{subdir}\"\n",
    "log = f\"{'-'.join(source_dir.split('/')[-3:])}-files.csv\"\n",
    "destination_dir = f\"ip005-ras81-lsst-ir-fusion/{subdir}\" \n",
    "folders = []\n",
    "folder_files = []\n",
    "ncores = 1 # change to adjust number of CPUs (= number of concurrent connections)\n",
    "perform_checksum = True\n",
    "upload_checksum = False\n",
    "dryrun = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9c91ac5-e798-4a94-8149-a43bb75350ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add titles to log file\n",
    "with open(log, 'w') as logfile: # elsewhere open(log, 'a')\n",
    "    logfile.write('LOCAL_FOLDER,LOCAL_PATH,FILE_SIZE,BUCKET_NAME,DESTINATION_KEY,CHECKSUM,CHECKSUM_SIZE,CHECKSUM_KEY\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2b2f300e-12d4-4519-a5d9-042325d0f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bucket\n",
    "s3_host = 'echo.stfc.ac.uk'\n",
    "keys = bm.get_keys(os.sep.join([os.environ['HOME'],'lsst_keys.json']))\n",
    "access_key = keys['access_key']\n",
    "secret_key = keys['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "56867cc2-4584-461e-b74c-0a544f93d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = bm.get_resource(access_key, secret_key, s3_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "36e800f7-eb7c-41af-aeee-5e16b4620a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.ServiceResource()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4b78d323-91e0-48f5-b219-2192899c4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'csd3-backup-test'\n",
    "if dryrun:\n",
    "    mybucket = 'dummy_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ed96d5b-27c7-4e22-aa83-7ab8e615332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_list = bm.bucket_list(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8def93c7-815e-4d5d-a469-0ed5d84dfb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSST-IR-FUSION',\n",
       " 'LSST-IR-FUSION_gen3_conversion',\n",
       " 'dmu4',\n",
       " 'lsst-dac',\n",
       " 'lsst-drp-config',\n",
       " 'lsst-test',\n",
       " 'lsst-test-3',\n",
       " 'lsst-test2']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d2861a41-1092-42cd-89f6-2b63671cca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added bucket: csd3-backup-test\n"
     ]
    }
   ],
   "source": [
    "if bucket_name not in bucket_list:\n",
    "    if not dryrun:\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "            print(f'Added bucket: {bucket_name}')\n",
    "else:\n",
    "    if not dryrun:\n",
    "        print(f'Bucket exists: {bucket_name}')\n",
    "        #sys.exit('Bucket exists.')\n",
    "    else:\n",
    "        print(f'Bucket exists: {bucket_name}')\n",
    "        print('dryrun = True, so continuing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6fef8793-7c2a-4d59-98dc-044ca6f077e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a538714e-5c1d-42b2-8424-6dba98ba4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.print_objects(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0545b84-49bb-40f5-b2c0-bd1f53081b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9fdf2e0-2db2-408c-ad63-bd6ebf75eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket(s3_host,access_key,secret_key,bucket_name,folder,filename,object_key,perform_checksum,upload_checksum,dryrun):\n",
    "    s3 = bm.get_resource(access_key, secret_key, s3_host)\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    file_data = open(filename, 'rb')\n",
    "    if perform_checksum:\n",
    "        \"\"\"\n",
    "        - Create checksum object\n",
    "        \"\"\"\n",
    "        checksum = hashlib.md5(file_data.read()).hexdigest().encode('utf-8')\n",
    "        if upload_checksum and not dryrun:\n",
    "            checksum_key = object_key + '.checksum'\n",
    "            #create checksum object\n",
    "            bucket.put_object(body=checksum,ContentEncoding='utf-8',Key=checksum_key)\n",
    "    \"\"\"\n",
    "    - Upload the file to the bucket\n",
    "    \"\"\"\n",
    "    if not dryrun:\n",
    "        bucket.upload_fileobj(file_data,object_key)\n",
    "\n",
    "    \"\"\"\n",
    "        report actions\n",
    "        CSV formatted\n",
    "        header: LOCAL_FOLDER,LOCAL_PATH,FILE_SIZE,BUCKET_NAME,DESTINATION_KEY,CHECKSUM,CHECKSUM_SIZE,CHECKSUM_KEY\n",
    "    \"\"\"\n",
    "    return_string = f'{folder},{filename},{os.stat(filename).st_size},{bucket_name},{object_key}'\n",
    "    if perform_checksum and upload_checksum:\n",
    "        return_string += f',{checksum},{len(checksum)},{checksum_key}'\n",
    "    elif perform_checksum:\n",
    "        return_string += f',{checksum},n/a,n/a'\n",
    "    else:\n",
    "        return_string += ',n/a,n/a,n/a'\n",
    "    return return_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b1edc6ff-44de-4339-8f36-9bec9d880205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(log,folder,file_count,total_size,folder_start,folder_end,upload_checksum):\n",
    "    elapsed = folder_end - folder_start\n",
    "    print(f'Finished folder {folder}, elapsed time = {elapsed}')\n",
    "    elapsed_seconds = elapsed.seconds + elapsed.microseconds / 1e6\n",
    "    avg_file_size = total_size / file_count / 1024**2\n",
    "    if not upload_checksum:\n",
    "        print(f'{file_count} files (avg {avg_file_size:.2f} MiB/file) uploaded in {elapsed_seconds:.2f} seconds, {elapsed_seconds/file_count:.2f} s/file',flush=True)\n",
    "        print(f'{total_size / 1024**2:.2f} MiB uploaded in {elapsed_seconds:.2f} seconds, {total_size / 1024**2 / elapsed_seconds:.2f} MiB/s',flush=True)\n",
    "    if upload_checksum:\n",
    "        checksum_size = 32*file_count # checksum byte strings are 32 bytes\n",
    "        total_size += checksum_size\n",
    "        file_count *= 2\n",
    "        print(f'{file_count} files (avg {avg_file_size:.2f} MiB/file) uploaded (including checksum files) in {elapsed_seconds:.2f} seconds, {elapsed_seconds/file_count:.2f} s/file',flush=True)\n",
    "        print(f'{total_size / 1024**2:.2f} MiB uploaded (including checksum files) in {elapsed_seconds:.2f} seconds, {total_size / 1024**2 / elapsed_seconds:.2f} MiB/s',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "718eacea-e758-404d-a753-5178d3475055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(s3_host,access_key,secret_key, bucket_name, current_objects, source_dir, destination_dir, ncores, perform_checksum, upload_checksum, dryrun, log):\n",
    "    i = 0\n",
    "    #processed_files = []\n",
    "    with Pool(ncores) as pool: # use 4 CPUs by default - very little speed-up, might drop multiprocessing and parallelise at shell level\n",
    "        #recursive loop over local folder\n",
    "        for folder,subfolders,files in os.walk(source_dir):\n",
    "            # check folder isn't empty\n",
    "            if len(files) > 0:\n",
    "                # all files within folder\n",
    "                folder_files = [ os.sep.join([folder,filename]) for filename in files ]\n",
    "                # keys to files on s3\n",
    "                object_names = [ os.sep.join([destination_dir, os.path.relpath(filename, source_dir)]) for filename in folder_files ]\n",
    "                print(f'folder_files: {folder_files}')\n",
    "                print(f'object_names: {object_names}')\n",
    "                init_len = len(object_names)\n",
    "                # remove current objects - avoids reuploading\n",
    "                # could provide overwrite flag if this is desirable\n",
    "                print(f'current_objects: {current_objects}')\n",
    "                if all([obj in current_objects for obj in object_names]):\n",
    "                    #all files in this subfolder already in bucket\n",
    "                    print(f'Skipping subfoler - all files exist.')\n",
    "                    continue\n",
    "                for oni,on in enumerate(object_names):\n",
    "                    if on in current_objects:\n",
    "                        object_names.remove(on)\n",
    "                        del folder_files[oni]\n",
    "                file_count = len(object_names)\n",
    "                if init_len - file_count > 0:\n",
    "                    print(f'Skipping {init_len - file_count} existing files.')\n",
    "                print(f'folder_files: {folder_files}')\n",
    "                print(f'object_names: {object_names}')\n",
    "                folder_start = datetime.now()\n",
    "                \n",
    "                print('check for symlinks')\n",
    "                for f in files:\n",
    "                    if os.path.islink(f):\n",
    "                        print(os.path.islink(f))\n",
    "                        raise Exception(\"Not dealing with symlinks here yet.\")\n",
    "                # upload files in parallel and log output\n",
    "                print(f'Uploading {file_count} files from {folder} using {ncores} processes.')\n",
    "                with open(log, 'a') as logfile:\n",
    "                    for result in pool.starmap(upload_to_bucket, zip(repeat(s3_host),repeat(access_key),repeat(secret_key), repeat(bucket_name), repeat(folder), folder_files, object_names, repeat(perform_checksum), repeat(upload_checksum), repeat(dryrun))):\n",
    "                        logfile.write(f'{result}\\n')\n",
    "                folder_end = datetime.now()\n",
    "                folder_files_size = np.sum(np.array([os.path.getsize(filename) for filename in folder_files]))\n",
    "                print_stats(log, folder, file_count, folder_files_size, folder_start, folder_end, upload_checksum)\n",
    "\n",
    "                # testing - stop after 1 folders\n",
    "                # i+=1\n",
    "                # if i == 100:\n",
    "                #     break\n",
    "            else:\n",
    "                print(f'Skipping subfoler - empty.')\n",
    "    # Upload log file\n",
    "    if not dryrun:\n",
    "        upload_to_bucket(s3_host,access_key,secret_key,bucket_name, '/', log, os.path.basename(log), False, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d857d-b6aa-44d4-980c-fa50949c76b4",
   "metadata": {},
   "source": [
    "# Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a14b8604-0122-46d9-91ff-68e93ccb3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = [1,2,3,4,5,6,7]\n",
    "c = [5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0fac5cbb-e8a0-45c2-aeb8-fb61d72bb85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x in b for x in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "172acda1-1c64-4a26-abea-68f1915c5a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([x in c for x in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69e0540d-fef9-4f09-a106-fe0e2e718e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'tx0000000000000077cbeb5-0065dc830b-2d5dc2782-default',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-request-id': 'tx0000000000000077cbeb5-0065dc830b-2d5dc2782-default',\n",
       "   'date': 'Mon, 26 Feb 2024 12:24:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.Object(bucket_name,'ip005-ras81-lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_HSC-I_2.0as_IRAC2.8as_2020_05_26.fits').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb7c6502-ac58-4aa7-bc1b-7e5d92438bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at 2024-02-26 12:24:43.792610, elapsed time = 0:00:01.686462\n",
      "folder_files: ['/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/readme.md']\n",
      "object_names: ['ip005-ras81-lsst-ir-fusion/dmu3/readme.md']\n",
      "current_objects: []\n",
      "folder_files: ['/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/readme.md']\n",
      "object_names: ['ip005-ras81-lsst-ir-fusion/dmu3/readme.md']\n",
      "check for symlinks\n",
      "Uploading 1 files from /rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3 using 1 processes.\n",
      "Finished folder /rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3, elapsed time = 0:00:00.494064\n",
      "1 files (avg 0.00 MiB/file) uploaded in 0.49 seconds, 0.49 s/file\n",
      "0.00 MiB uploaded in 0.49 seconds, 0.00 MiB/s\n",
      "folder_files: ['/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_HSC-I_2.0as_IRAC2.8as_2020_05_26.fits', '/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_Ks_2.0as_IRAC2.8as_2020_06_01.fits']\n",
      "object_names: ['ip005-ras81-lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_HSC-I_2.0as_IRAC2.8as_2020_05_26.fits', 'ip005-ras81-lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_Ks_2.0as_IRAC2.8as_2020_06_01.fits']\n",
      "current_objects: []\n",
      "folder_files: ['/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_HSC-I_2.0as_IRAC2.8as_2020_05_26.fits', '/rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_Ks_2.0as_IRAC2.8as_2020_06_01.fits']\n",
      "object_names: ['ip005-ras81-lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_HSC-I_2.0as_IRAC2.8as_2020_05_26.fits', 'ip005-ras81-lsst-ir-fusion/dmu3/data/XMMFULL_DR2_MASKVISTA_Ks_2.0as_IRAC2.8as_2020_06_01.fits']\n",
      "check for symlinks\n",
      "Uploading 2 files from /rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/data using 1 processes.\n",
      "Finished folder /rds/project/rds-rPTGgs6He74/ras81/lsst-ir-fusion/dmu3/data, elapsed time = 0:00:06.955877\n",
      "2 files (avg 920.18 MiB/file) uploaded in 6.96 seconds, 3.48 s/file\n",
      "1840.35 MiB uploaded in 6.96 seconds, 264.57 MiB/s\n",
      "Finished at 2024-02-26 12:24:52.053035, elapsed time = 0:00:09.946890\n"
     ]
    }
   ],
   "source": [
    "# Process the files in parallel\n",
    "current_objects = bm.object_list(bucket)\n",
    "current_objects\n",
    "print(f'Starting processing at {datetime.now()}, elapsed time = {datetime.now() - start}')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    process_files(s3_host,access_key,secret_key, bucket_name, current_objects, source_dir, destination_dir, ncores, perform_checksum, upload_checksum, dryrun, log)\n",
    "\n",
    "# Complete\n",
    "print(f'Finished at {datetime.now()}, elapsed time = {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f18f84-4616-46b2-b069-de01b8838a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
